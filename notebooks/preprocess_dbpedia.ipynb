{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_dbpedia.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNbDU528Po+w/60EuK7LpUu"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GgH7fzsPUtHN"},"source":["## Mounting Google Drive to access data\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"cRKr0MTRIFCP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607711090996,"user_tz":300,"elapsed":15531,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}},"outputId":"a1e56a95-9e05-491e-bea7-89faeafebb74"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OlDHfWAaU98q"},"source":["## Loading the data"]},{"cell_type":"code","metadata":{"id":"pCHXA_X7LkXe","executionInfo":{"status":"ok","timestamp":1607711092504,"user_tz":300,"elapsed":607,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}}},"source":["DATA_DIR = '/content/drive/My Drive/AML_Project/dbpedia_csv'\n","BIN_DIR = '/content/drive/My Drive/AML_Project/binaries'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqNQQdwbIVGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607711099677,"user_tz":300,"elapsed":5569,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}},"outputId":"1eca52f9-3649-4ea8-89e9-a24ec7d0539a"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","train_data_path = os.path.join(DATA_DIR, 'train.csv')\n","test_data_path = os.path.join(DATA_DIR, 'test.csv')\n","\n","train_df = pd.read_csv(train_data_path, header=None, names=['label', 'title', 'text'])\n","print(train_df.shape)\n","test_df = pd.read_csv(test_data_path, header=None, names=['label', 'title', 'text'])\n","print(test_df.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(560000, 3)\n","(70000, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nRRc9DDnt9Ju"},"source":["# Reducing the size of the data\n","We are reducing the dataset size to reduce the compute intensive nature of the algorithms that we are going to be working with.\n","* Number of classes = 3 (reduced from 14 in the entire dataset)\n","* Training size = 7.5k data points (2.5k for each class)\n","* Testing size = 2.25k data points (750 for each class\n"]},{"cell_type":"code","metadata":{"id":"8nMmIFc4vlV8"},"source":["num_classes = 3\n","train_size_per_class = 2500\n","test_size_per_class = 750\n","\n","cols = ['label', 'text']\n","\n","def reduce_dataframe(df, num_per_class):\n","    reduce_df = pd.DataFrame(columns=cols)\n","    labels = np.unique(df.label)[:num_classes]\n","    for l in labels:\n","        l_idx = np.where(df.label == l)[0]\n","        small_idx = random.sample(l_idx.tolist(), num_per_class)\n","        small_df = pd.DataFrame(df[cols].iloc[small_idx])\n","        reduce_df = reduce_df.append(small_df)\n","\n","    return reduce_df\n","\n","small_train_df = reduce_dataframe(train_df, train_size_per_class)\n","small_test_df = reduce_dataframe(test_df, test_size_per_class)\n","\n","small_train_df.to_csv(os.path.join(DATA_DIR, 'small_train.csv'), index=False, header=False)\n","small_test_df.to_csv(os.path.join(DATA_DIR, 'small_test.csv'), index=False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JvTcowXVCo7"},"source":["# Preprocessing text"]},{"cell_type":"code","metadata":{"id":"u63DCDBhI7ZT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607657146534,"user_tz":300,"elapsed":2199,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}},"outputId":"bfe1cdc3-59fa-4ced-e068-ee7c2846cf2e"},"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"MTON9sDqJFou"},"source":["import re\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords \n","\n","def preprocess_text(text):\n","    # removing numbers\n","    text = re.sub('[0-9]+', '', text)\n","    \n","    # removing urls\n","    text = re.sub(r'http\\S+', '', text)\n","    \n","    # removing punctuation and special characters\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokens = tokenizer.tokenize(text)\n","    \n","    # convert to lowercase and lemmatize\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(token.lower(), pos='v') for token in tokens]\n","    \n","    # remove stop words\n","    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n","    \n","    # remove small words\n","    keywords = [word for word in keywords if len(word) > 2]\n","    \n","    return keywords\n","\n","small_train_df['preprocess_text'] = small_train_df.text.apply(preprocess_text)\n","small_test_df['preprocess_text'] = small_test_df.text.apply(preprocess_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZXJzjfUMyM-"},"source":["Saving the preprocessed text to pkl file"]},{"cell_type":"code","metadata":{"id":"MMQRfwggM3o2"},"source":["small_train_df.to_pickle(os.path.join(BIN_DIR, 'small_train_preprocessed.pkl'))\n","small_test_df.to_pickle(os.path.join(BIN_DIR, 'small_test_preprocessed.pkl'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yX2Kc31kVMt2"},"source":["# Generating word embeddings"]},{"cell_type":"code","metadata":{"id":"5RPLEKlJJIVA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607657977439,"user_tz":300,"elapsed":24621,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}},"outputId":"7e90fbf1-fd88-41a7-c15e-6dce2e7cefb0"},"source":["from gensim.models import Word2Vec\n","\n","all_text = pd.concat([small_train_df.preprocess_text, small_test_df.preprocess_text], axis=0)\n","w2v_model = Word2Vec(sentences=all_text, size=300, min_count=1, window=5, workers=4, sg=1)\n","w2v_model.wv.vectors.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(37424, 300)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"_vzRVRvlmgsA"},"source":["Saving the word embeddings"]},{"cell_type":"code","metadata":{"id":"NqpZpMZxJXNu"},"source":["w2v_model.wv.save(os.path.join(BIN_DIR, 'small_dbpedia.wordembeddings'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIKufmOtmvjJ"},"source":["# Generating word vectors for text\n","Loading word embeddings"]},{"cell_type":"code","metadata":{"id":"fUFOYve9V0LE","executionInfo":{"status":"ok","timestamp":1607711109914,"user_tz":300,"elapsed":4675,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors\n","\n","small_train_df = pd.read_pickle(os.path.join(BIN_DIR, 'small_train_preprocessed.pkl'))\n","small_test_df = pd.read_pickle(os.path.join(BIN_DIR, 'small_test_preprocessed.pkl'))\n","\n","word_vectors = KeyedVectors.load(os.path.join(BIN_DIR, 'small_dbpedia.wordembeddings'), mmap='r')\n","\n","def vectorize_text(text, wv):\n","    vec = np.zeros((1, 300))\n","    for w in text:\n","        vec += wv.get_vector(w)\n","\n","    return vec / len(text)\n","\n","small_train_df['text_vec'] = small_train_df.preprocess_text.apply(vectorize_text, args=(word_vectors,))\n","small_test_df['text_vec'] = small_test_df.preprocess_text.apply(vectorize_text, args=(word_vectors,)) "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxIACE7qKQ8P","executionInfo":{"status":"ok","timestamp":1607711115005,"user_tz":300,"elapsed":1740,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}}},"source":["train_vec = pd.DataFrame(small_train_df.text_vec.explode().tolist())\n","test_vec = pd.DataFrame(small_test_df.text_vec.explode().tolist())"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ReS9PAxdDNUq","executionInfo":{"status":"ok","timestamp":1607711575862,"user_tz":300,"elapsed":6217,"user":{"displayName":"Sailesh Patnala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSlSbsOKVx_TM3KcnwJBONcxaQcoX-eFBXzMHXsw=s64","userId":"01820300511168194889"}}},"source":["pd.DataFrame(train_vec).to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_all_x.pkl'))\n","pd.DataFrame(small_train_df.label).to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_all_y.pkl'))\n","\n","train_vec['label'] = small_train_df.label.values\n","test_vec['label'] = small_test_df.label.values\n","train_vec.to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_wv.pkl'))\n","test_vec.to_pickle(os.path.join(BIN_DIR, 'dbpedia_test_wv.pkl'))"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bDDhr8Npq3CD"},"source":["# Generating label splits\n","We're generating multiple datasets with different proportions of the training data that is labelled and unlabelled. We will be using these splits to benchmark the performance of our semi-supervised learning model:\n","* Label Split 1: 125 data pts / class (5% of dataset)\n","* Label Split 2: 75 data pts / class (3% dataset)"]},{"cell_type":"code","metadata":{"id":"4ML6aGIn7QvQ"},"source":["num_per_class_split1 = 125\n","num_per_class_split2 = 75"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQHCdc91Oajj"},"source":["import random\n","\n","def generate_label_splits_pkl(train_df, train_vec):\n","    \"\"\"\n","    Function to generate the label splits\n","    \"\"\"\n","    # final dataframes\n","    x_split1_df, y_split1_df = pd.DataFrame(), pd.DataFrame()\n","    x_split2_df, y_split2_df = pd.DataFrame(), pd.DataFrame()\n","\n","    labels = np.unique(train_df.label)\n","    for l in labels:\n","        l_idx = np.where(train_df.label == l)[0]\n","        # generating split 1\n","        split1_idx = random.sample(l_idx.tolist(), num_per_class_split1)\n","        x_split1_df = x_split1_df.append(pd.DataFrame(train_vec.iloc[split1_idx]))\n","        y_split1_df = y_split1_df.append(pd.DataFrame(train_df.label.iloc[split1_idx]))\n","\n","        # generating split 2\n","        split2_idx = random.sample(l_idx.tolist(), num_per_class_split2)\n","        x_split2_df = x_split2_df.append(pd.DataFrame(train_vec.iloc[split2_idx]))\n","        y_split2_df = y_split2_df.append(pd.DataFrame(train_df.label.iloc[split2_idx]))\n","\n","    x_split1_df.to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_x_split1.pkl'))\n","    y_split1_df.to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_y_split1.pkl'))\n","    x_split2_df.to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_x_split2.pkl'))\n","    y_split2_df.to_pickle(os.path.join(BIN_DIR, 'dbpedia_train_y_split2.pkl'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"op-823juOibh"},"source":["small_train_df = pd.read_csv(os.path.join(DATA_DIR, 'small_train.csv'), names=['label', 'text'])\n","train_vec = pd.read_pickle(os.path.join(BIN_DIR, 'dbpedia_train_all_x.pkl'))\n","generate_label_splits_pkl(small_train_df, train_vec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIqoY4Dmrqg8"},"source":["import random\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","def generate_label_splits_csv(df):\n","    \"\"\"\n","    Function to generate the label splits\n","    \"\"\"\n","    # final dataframes\n","    cols=['label', 'text']\n","    split1_df = pd.DataFrame(columns=cols)\n","    train_split1_df = pd.DataFrame(columns=cols)\n","    validation_split1_df = pd.DataFrame(columns=cols)\n","\n","    split2_df = pd.DataFrame(columns=cols)\n","    train_split2_df = pd.DataFrame(columns=cols)\n","    validation_split2_df = pd.DataFrame(columns=cols)\n","\n","    labels = np.unique(df.label)\n","    for l in labels:\n","        l_idx = np.where(df.label == l)[0]\n","        # generating split 1\n","        split1_idx = random.sample(l_idx.tolist(), num_per_class_split1)\n","        split1_df = pd.DataFrame(df[cols].iloc[split1_idx])\n","        train, validation = train_test_split(split1_df)\n","        train_split1_df = train_split1_df.append(train)\n","        validation_split1_df = validation_split1_df.append(validation)\n","        \n","        # generating split 2\n","        split2_idx = random.sample(l_idx.tolist(), num_per_class_split2)\n","        split2_df = pd.DataFrame(df[cols].iloc[split2_idx])\n","        train, validation = train_test_split(split2_df)\n","        train_split2_df = train_split2_df.append(train)\n","        validation_split2_df = validation_split2_df.append(validation)\n","\n","    train_split1_df.to_csv(os.path.join(DATA_DIR, 'dbpedia_train_split1.csv'), index=False, header=False)\n","    validation_split1_df.to_csv(os.path.join(DATA_DIR, 'dbpedia_validation_split1.csv'), index=False, header=False)\n","    train_split2_df.to_csv(os.path.join(DATA_DIR, 'dbpedia_train_split2.csv'), index=False, header=False)\n","    validation_split2_df.to_csv(os.path.join(DATA_DIR, 'dbpedia_validation_split2.csv'), index=False, header=False)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls9G6v_WaW8b"},"source":["small_train_df = pd.read_csv(os.path.join(DATA_DIR, 'small_train.csv'), header=None, names=['label', 'text'])\n","generate_label_splits_csv(small_train_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZDmXFeHUJPN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPHJWRq7oiUp"},"source":[""]}]}