{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data from pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_split1 = pd.read_pickle('data/dbpedia_train_x_split1.pkl')\n",
    "y_split1 = pd.read_pickle('data/dbpedia_train_y_split1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38018</th>\n",
       "      <td>-0.005324</td>\n",
       "      <td>0.205206</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>-0.309079</td>\n",
       "      <td>-0.109366</td>\n",
       "      <td>-0.258832</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>0.086448</td>\n",
       "      <td>0.055326</td>\n",
       "      <td>0.220757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169310</td>\n",
       "      <td>0.138412</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>-0.044117</td>\n",
       "      <td>0.055317</td>\n",
       "      <td>0.187381</td>\n",
       "      <td>-0.098321</td>\n",
       "      <td>-0.217412</td>\n",
       "      <td>-0.220616</td>\n",
       "      <td>0.182205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10389</th>\n",
       "      <td>0.255242</td>\n",
       "      <td>0.188798</td>\n",
       "      <td>-0.135879</td>\n",
       "      <td>-0.207213</td>\n",
       "      <td>-0.214796</td>\n",
       "      <td>-0.272929</td>\n",
       "      <td>0.200540</td>\n",
       "      <td>0.154624</td>\n",
       "      <td>-0.109058</td>\n",
       "      <td>0.141196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181943</td>\n",
       "      <td>-0.035217</td>\n",
       "      <td>-0.184698</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>0.115205</td>\n",
       "      <td>0.156353</td>\n",
       "      <td>-0.224457</td>\n",
       "      <td>-0.114330</td>\n",
       "      <td>-0.198991</td>\n",
       "      <td>-0.007543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37016</th>\n",
       "      <td>0.113450</td>\n",
       "      <td>-0.009364</td>\n",
       "      <td>-0.131690</td>\n",
       "      <td>-0.256048</td>\n",
       "      <td>-0.153683</td>\n",
       "      <td>-0.168570</td>\n",
       "      <td>0.108866</td>\n",
       "      <td>0.078384</td>\n",
       "      <td>-0.041223</td>\n",
       "      <td>0.151630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084727</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>-0.026646</td>\n",
       "      <td>0.144471</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>-0.141484</td>\n",
       "      <td>-0.089492</td>\n",
       "      <td>-0.131767</td>\n",
       "      <td>0.109087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>0.104002</td>\n",
       "      <td>0.051275</td>\n",
       "      <td>-0.143594</td>\n",
       "      <td>-0.222141</td>\n",
       "      <td>-0.112486</td>\n",
       "      <td>-0.314900</td>\n",
       "      <td>0.167336</td>\n",
       "      <td>0.121801</td>\n",
       "      <td>0.031601</td>\n",
       "      <td>0.242447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085333</td>\n",
       "      <td>0.088219</td>\n",
       "      <td>-0.251650</td>\n",
       "      <td>0.030851</td>\n",
       "      <td>0.090480</td>\n",
       "      <td>0.315288</td>\n",
       "      <td>-0.136301</td>\n",
       "      <td>-0.181215</td>\n",
       "      <td>-0.329499</td>\n",
       "      <td>0.237604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>0.156203</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>-0.010680</td>\n",
       "      <td>-0.233068</td>\n",
       "      <td>-0.241265</td>\n",
       "      <td>-0.173814</td>\n",
       "      <td>0.221157</td>\n",
       "      <td>0.192151</td>\n",
       "      <td>-0.029584</td>\n",
       "      <td>0.185003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111471</td>\n",
       "      <td>0.133750</td>\n",
       "      <td>-0.208221</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>0.082757</td>\n",
       "      <td>0.259925</td>\n",
       "      <td>-0.131668</td>\n",
       "      <td>-0.198523</td>\n",
       "      <td>-0.185508</td>\n",
       "      <td>0.178424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521492</th>\n",
       "      <td>0.321894</td>\n",
       "      <td>0.364019</td>\n",
       "      <td>-0.028605</td>\n",
       "      <td>-0.211936</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>-0.210255</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>0.304995</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055579</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>-0.190833</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.090201</td>\n",
       "      <td>0.270010</td>\n",
       "      <td>-0.156809</td>\n",
       "      <td>-0.136904</td>\n",
       "      <td>-0.194771</td>\n",
       "      <td>0.115096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536501</th>\n",
       "      <td>0.202290</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>-0.336992</td>\n",
       "      <td>-0.316965</td>\n",
       "      <td>-0.079169</td>\n",
       "      <td>0.241228</td>\n",
       "      <td>0.240695</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>0.152834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028027</td>\n",
       "      <td>0.117409</td>\n",
       "      <td>-0.205209</td>\n",
       "      <td>0.070434</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>0.287230</td>\n",
       "      <td>-0.089631</td>\n",
       "      <td>-0.252135</td>\n",
       "      <td>-0.178748</td>\n",
       "      <td>0.224248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551031</th>\n",
       "      <td>0.167308</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>-0.031282</td>\n",
       "      <td>-0.265090</td>\n",
       "      <td>-0.227614</td>\n",
       "      <td>-0.096121</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>0.175551</td>\n",
       "      <td>0.083510</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084468</td>\n",
       "      <td>0.134981</td>\n",
       "      <td>-0.147401</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.119402</td>\n",
       "      <td>0.338470</td>\n",
       "      <td>-0.111547</td>\n",
       "      <td>-0.412538</td>\n",
       "      <td>-0.243889</td>\n",
       "      <td>0.096841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530237</th>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.103001</td>\n",
       "      <td>-0.042302</td>\n",
       "      <td>-0.272961</td>\n",
       "      <td>-0.263882</td>\n",
       "      <td>-0.077211</td>\n",
       "      <td>0.211424</td>\n",
       "      <td>0.158734</td>\n",
       "      <td>0.089401</td>\n",
       "      <td>0.133178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069092</td>\n",
       "      <td>0.073285</td>\n",
       "      <td>-0.195081</td>\n",
       "      <td>0.074679</td>\n",
       "      <td>0.113065</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>-0.052555</td>\n",
       "      <td>-0.251632</td>\n",
       "      <td>-0.215212</td>\n",
       "      <td>0.143994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552471</th>\n",
       "      <td>0.284631</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>-0.043219</td>\n",
       "      <td>-0.350693</td>\n",
       "      <td>-0.249776</td>\n",
       "      <td>-0.113039</td>\n",
       "      <td>0.257305</td>\n",
       "      <td>0.241576</td>\n",
       "      <td>0.043955</td>\n",
       "      <td>0.161621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045876</td>\n",
       "      <td>0.036240</td>\n",
       "      <td>-0.289827</td>\n",
       "      <td>0.132436</td>\n",
       "      <td>0.095453</td>\n",
       "      <td>0.267211</td>\n",
       "      <td>-0.120250</td>\n",
       "      <td>-0.291636</td>\n",
       "      <td>-0.142323</td>\n",
       "      <td>0.136739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "38018  -0.005324  0.205206  0.008683 -0.309079 -0.109366 -0.258832  0.276842   \n",
       "10389   0.255242  0.188798 -0.135879 -0.207213 -0.214796 -0.272929  0.200540   \n",
       "37016   0.113450 -0.009364 -0.131690 -0.256048 -0.153683 -0.168570  0.108866   \n",
       "12221   0.104002  0.051275 -0.143594 -0.222141 -0.112486 -0.314900  0.167336   \n",
       "6690    0.156203  0.004777 -0.010680 -0.233068 -0.241265 -0.173814  0.221157   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "521492  0.321894  0.364019 -0.028605 -0.211936  0.001405 -0.210255  0.225507   \n",
       "536501  0.202290 -0.006832  0.077930 -0.336992 -0.316965 -0.079169  0.241228   \n",
       "551031  0.167308  0.015951 -0.031282 -0.265090 -0.227614 -0.096121  0.032917   \n",
       "530237  0.238980  0.103001 -0.042302 -0.272961 -0.263882 -0.077211  0.211424   \n",
       "552471  0.284631  0.020324 -0.043219 -0.350693 -0.249776 -0.113039  0.257305   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "38018   0.086448  0.055326  0.220757  ... -0.169310  0.138412 -0.108493   \n",
       "10389   0.154624 -0.109058  0.141196  ... -0.181943 -0.035217 -0.184698   \n",
       "37016   0.078384 -0.041223  0.151630  ... -0.084727  0.006616 -0.070600   \n",
       "12221   0.121801  0.031601  0.242447  ... -0.085333  0.088219 -0.251650   \n",
       "6690    0.192151 -0.029584  0.185003  ... -0.111471  0.133750 -0.208221   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "521492  0.304995 -0.010479  0.030767  ...  0.055579  0.000221 -0.190833   \n",
       "536501  0.240695  0.034314  0.152834  ... -0.028027  0.117409 -0.205209   \n",
       "551031  0.175551  0.083510  0.168240  ... -0.084468  0.134981 -0.147401   \n",
       "530237  0.158734  0.089401  0.133178  ... -0.069092  0.073285 -0.195081   \n",
       "552471  0.241576  0.043955  0.161621  ... -0.045876  0.036240 -0.289827   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "38018  -0.044117  0.055317  0.187381 -0.098321 -0.217412 -0.220616  0.182205  \n",
       "10389   0.020663  0.115205  0.156353 -0.224457 -0.114330 -0.198991 -0.007543  \n",
       "37016  -0.026646  0.144471  0.141350 -0.141484 -0.089492 -0.131767  0.109087  \n",
       "12221   0.030851  0.090480  0.315288 -0.136301 -0.181215 -0.329499  0.237604  \n",
       "6690    0.016590  0.082757  0.259925 -0.131668 -0.198523 -0.185508  0.178424  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "521492  0.020330  0.090201  0.270010 -0.156809 -0.136904 -0.194771  0.115096  \n",
       "536501  0.070434  0.028975  0.287230 -0.089631 -0.252135 -0.178748  0.224248  \n",
       "551031  0.013449  0.119402  0.338470 -0.111547 -0.412538 -0.243889  0.096841  \n",
       "530237  0.074679  0.113065  0.228045 -0.052555 -0.251632 -0.215212  0.143994  \n",
       "552471  0.132436  0.095453  0.267211 -0.120250 -0.291636 -0.142323  0.136739  \n",
       "\n",
       "[28000 rows x 300 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38018</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10389</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37016</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12221</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521492</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536501</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551031</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530237</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552471</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class\n",
       "38018       1\n",
       "10389       1\n",
       "37016       1\n",
       "12221       1\n",
       "6690        1\n",
       "...       ...\n",
       "521492     14\n",
       "536501     14\n",
       "551031     14\n",
       "530237     14\n",
       "552471     14\n",
       "\n",
       "[28000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A torch implementation of VAT.\n",
    "import contextlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _disable_tracking_bn_stats(model):\n",
    "    def switch_attr(m):\n",
    "        if hasattr(m, 'track_running_stats'):\n",
    "            m.track_running_stats ^= True\n",
    "    model.apply(switch_attr)\n",
    "    yield\n",
    "    model.apply(switch_attr)\n",
    "    \n",
    "def normalize(d):\n",
    "    d /= (torch.sqrt(torch.sum(d**2, axis=1)).view(-1,1)+1e-16)\n",
    "    return d\n",
    "\n",
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-16\n",
    "    return d\n",
    "\n",
    "def _kl_div(p,q):\n",
    "    '''\n",
    "    D_KL(p||q) = Sum(p log p - p log q)\n",
    "    '''\n",
    "    logp = torch.nn.functional.log_softmax(p,dim=1)\n",
    "    logq = torch.nn.functional.log_softmax(q,dim=1)\n",
    "    p = torch.exp(logp)\n",
    "    return (p*(logp-logq)).sum(dim=1,keepdim=True).mean()\n",
    "\n",
    "class VATLoss(nn.Module):\n",
    "    def __init__(self, xi = .0001, eps = 0.1, ip = 2):\n",
    "        \"\"\"\n",
    "        :xi: hyperparameter: small float for finite difference threshold \n",
    "        :eps: hyperparameter: value for how much to deviate from original X.\n",
    "        :ip: value of power iteration for approximation of r_vadv.\n",
    "        \"\"\"\n",
    "        super(VATLoss, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.ip = ip\n",
    "        \n",
    "    def forward(self, model, x):\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "        \n",
    "        # random unit tensor for perturbation\n",
    "        d = torch.randn(x.shape)\n",
    "        d = _l2_normalize(d)\n",
    "        #calculating adversarial direction\n",
    "\n",
    "        for _ in range(self.ip):\n",
    "            d.requires_grad_()\n",
    "            pred_hat = model(x + self.xi * d)\n",
    "            adv_distance = _kl_div(pred_hat, pred)\n",
    "            adv_distance.backward()\n",
    "            d = _l2_normalize(d.grad.data)\n",
    "            model.zero_grad()\n",
    "        \n",
    "        r_adv = d*self.eps\n",
    "        pred_hat = model(x+r_adv)\n",
    "        lds = _kl_div(pred_hat, pred)\n",
    "        return lds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(300,100)\n",
    "        self.linear2 = nn.Linear(100,100)\n",
    "        self.linear3 = nn.Linear(100,100)\n",
    "        self.linear4 = nn.Linear(100,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(u):\n",
    "    vat_loss=VATLoss(ip=2,xi=0.5,eps=0.5)\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    lds = vat_loss(model,u.float())\n",
    "    output = model(torch.tensor(Xl_new).float())\n",
    "    classification_loss = cross_entropy(output, torch.tensor(yl_new))\n",
    "    loss = classification_loss + 4.*lds\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return classification_loss, lds\n",
    "\n",
    "def valid(win=None):\n",
    "    model.eval()\n",
    "    z_pred = torch.nn.functional.softmax(model(torch.from_numpy(X_all).float()),dim=1).data.numpy()[:,1]\n",
    "    val = np.c_[X_all,z_pred.T]\n",
    "    \n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    ax = fig.add_subplot(111)\n",
    "    h = ax.scatter(X_all[:,0],X_all[:,1], c=z_pred, vmin=0, vmax=1,cmap='seismic')\n",
    "    ax.scatter(X0l[:,0],X0l[:,1],c='C0',marker='s',s=100)\n",
    "    ax.scatter(X1l[:,0],X1l[:,1],c='C1',marker='v',s=100)\n",
    "    fig.colorbar(h)\n",
    "    return win,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.read_pickle('data/dbpedia_train_all_x.pkl')\n",
    "y_all = pd.read_pickle('data/dbpedia_train_all_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled = pd.read_pickle('data/dbpedia_train_x_split1.pkl')\n",
    "y_labeled = pd.read_pickle('data/dbpedia_train_y_split1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  1,  ..., 14, 14, 14])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(y_labeled.transpose().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1,  1,  ..., 14, 14, 14])\n",
      "tensor([[ 0.0615, -0.0452],\n",
      "        [ 0.0532, -0.0469],\n",
      "        [ 0.0537, -0.0451],\n",
      "        ...,\n",
      "        [ 0.0560, -0.0437],\n",
      "        [ 0.0556, -0.0496],\n",
      "        [ 0.0582, -0.0435]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saileshpatnala/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b1c3513d5d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mclassification_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "i_total_step = 0\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, eps=1e-3)\n",
    "gamma = 4e1\n",
    "\n",
    "var = torch.autograd.Variable\n",
    "# ftn = torch.FloatTensor\n",
    "# ltn = torch.LongTensor\n",
    "X_all_tensor = torch.from_numpy(X_all.values).float()\n",
    "y_all_tensor = torch.from_numpy(y_all.values).long()\n",
    "X_labeled_tensor = torch.from_numpy(X_labeled.values).float()\n",
    "y_labeled_tensor = torch.from_numpy(y_labeled.transpose().values[0])\n",
    "dataset = torch.utils.data.TensorDataset(X_all_tensor, y_all_tensor)\n",
    "\n",
    "print(y_labeled_tensor)\n",
    "\n",
    "num_epochs = 150\n",
    "for i in range(num_epochs):#epoch\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, 121, shuffle=True) #batch size \n",
    "    for u, _ in data_loader:\n",
    "        i_total_step += 1\n",
    "        vat_loss = VATLoss()\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        lds = vat_loss(model, torch.tensor(u).float())\n",
    "        output = model(X_labeled_tensor)\n",
    "        print(output)\n",
    "        classification_loss = cross_entropy(output, y_labeled_tensor)\n",
    "        loss = classification_loss + gamma * lds\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy = accuracy_score(y_labeled,np.argmax(output.data.numpy(),axis=1))\n",
    "        ce_losses = classification_loss.item()\n",
    "        vat_losses = gamma*lds.item()\n",
    "        if i_total_step % 200 == 0:\n",
    "            print(\"CrossEntropyLoss %f:\" % (ce_losses))\n",
    "            print(\"VATLoss %f:\" % (vat_losses))\n",
    "            print(\"Accuracy %f:\" % (accuracy)) \n",
    "            print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = torch.nn.functional.softmax(model(var(ftn(X_all))), dim=1).data.numpy()[:, 1]\n",
    "z_pred = np.where(z_pred > 0.5, 1, 0)\n",
    "print(\"{0} vat acc\".format(accuracy_score(y_all, z_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
